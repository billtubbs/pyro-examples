{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RODD disturbance model estimation using Variational Inference with Pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import (SVI, TraceEnum_ELBO, config_enumerate, \n",
    "                        infer_discrete)\n",
    "from pyro.infer.autoguide import AutoNormal\n",
    "\n",
    "smoke_test = ('CI' in os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.9'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.set_rng_seed(101)\n",
    "pyro.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = 'plots'\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.mkdir(plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring the parameters of a stochastic process with a discrete random decision variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a noisy measurement of a disturbance process that looks like a series of randomly-occurring step changes:\n",
    "\n",
    "<img src=\"images/Wong_and_Lee_fig1c_steps.png\" width=\"320\">\n",
    "\n",
    "<center> Figure source: Wong and Lee (2009)</center>\n",
    "\n",
    "We could generate a disturbance like this by integrating a 'random shock' variable which is sampled from either of two normal distributions, according to a discrete random variable $\\gamma(k)$ which is either 0 or 1:\n",
    "\n",
    "$$\n",
    "\\gamma(k) \\sim\n",
    "\\begin{cases}\n",
    "0 & \\text{with probability } 1-\\epsilon \\\\\n",
    "1 & \\text{with probability } \\epsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\epsilon<<1$.\n",
    "\n",
    "<img src=\"images/shock_dist_1.png\" width=\"320\">\n",
    "<img src=\"images/shock_dist_2.png\" width=\"320\">\n",
    "\n",
    "In general, the discrete-time model for this class of disturbances is:\n",
    "\n",
    "$$p(k) = \\frac{B(q^{-1})}{A(q^{-1})} w_p(k)$$\n",
    "\n",
    "This is known as a randomly-occurring deterministic disturbance (RODD) (MacGregor et al. 1984).\n",
    "\n",
    "Reference:\n",
    "\n",
    "- MacGregor, J.F., Harris, T.J., and Wright, J.D. (1984). Duality Between the Control of Processes Subject to Randomly Occurring Deterministic Disturbances and ARIMA Stochastic Disturbances. Technometrics, 26(4), 389–397.\n",
    "- Wong, W.C. and Lee, J.H. (2009). Realistic disturbance modeling using hidden Markov models: Applications in model-based process control. Journal of Process\n",
    "Control, 19(9), 1438–1450."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we construct a model of our belief about the process that generated the data:\n",
    "\n",
    "<img src=\"images/RODD_step_diag.png\" width=\"320\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random shock variable\n",
    "\n",
    "$$\n",
    "w_{p}(k) \\sim\n",
    "\\begin{cases}\n",
    "\\mathcal{N}(0, \\sigma_{w_p}^2) & \\text{with probability } 1-\\epsilon \\\\\n",
    "\\mathcal{N}(0, b^2 \\sigma_{w_p}^2) & \\text{with probability } \\epsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Disturbance generating process\n",
    "\n",
    "$$p(k)=\\frac{1}{1-z^{-1}}w_{p}(k)$$\n",
    "\n",
    "Measurement noise\n",
    "\n",
    "$$ v(k) \\sim \\mathcal{N}(0, \\sigma_{M}^2)$$\n",
    "\n",
    "or\n",
    "\n",
    "$$ y_m(k) \\sim \\mathcal{N}(p(k), \\sigma_{M}^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have initial guesses of the shock probability $\\epsilon$, the variance of the signal when there is no shock $\\sigma_{w_p}^2$, and when a shock occurs $b^2\\sigma_{w_p}^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_guess = 0.05\n",
    "b_guess = 50\n",
    "scale_guess = 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Define a stochastic function\n",
    "\n",
    " - See: http://pyro.ai/examples/intro_part_i.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Shock Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shock(scale_guess, b_guess, epsilon_guess):\n",
    "    # Scale, b and epsilon are always positive\n",
    "    scale = pyro.sample(\"scale\", dist.LogNormal(np.log(scale_guess), 0.25))\n",
    "    b = pyro.sample(\"b\", dist.LogNormal(np.log(b_guess), 0.25))\n",
    "    epsilon = pyro.sample(\"epsilon\", dist.LogNormal(np.log(epsilon_guess), 0.25))\n",
    "    alpha = pyro.sample(\"alpha\", dist.Bernoulli(epsilon))\n",
    "    alpha = alpha.long()\n",
    "    wp_dist = dist.Normal(torch.tensor([0.0, 0.0])[alpha], \n",
    "                          torch.tensor([scale, scale*b])[alpha])\n",
    "    measurement = pyro.sample('obs', wp_dist)\n",
    "    return measurement\n",
    "\n",
    "def infrequent_step_5(scale_guess, b_guess, epsilon_guess):\n",
    "    y = 0  # initial value\n",
    "    measurements = []\n",
    "    for k in range(5):\n",
    "        y = y + random_shock(scale_guess, b_guess, epsilon_guess)\n",
    "        measurements.append(y)\n",
    "    return torch.tensor(measurements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0156, 0.0179, 0.0519, 0.0739, 0.0711])\n",
      "tensor([ 1.7345e-04, -4.2217e-03, -6.6964e-05,  1.6579e-02, -3.5251e-02])\n",
      "tensor([ 0.0012, -0.0081,  0.0535, -0.2341, -0.1900])\n",
      "tensor([ 0.0189, -0.0123,  0.0166,  0.0029, -0.0133])\n",
      "tensor([-0.0015,  0.0066,  0.0482,  0.1065,  0.1311])\n"
     ]
    }
   ],
   "source": [
    "# Draw random samples\n",
    "for _ in range(5):\n",
    "    print(infrequent_step_5(scale_guess, b_guess, epsilon_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model conditioned on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infrequent_step_5_conditioned(scale_guess, b_guess, epsilon_guess, observations):\n",
    "    y = 0  # initial value\n",
    "    for k, y_m in enumerate(observations):\n",
    "        y = y + random_shock(scale_guess, b_guess, epsilon_guess)\n",
    "        pyro.sample(f\"y_{k}\", y, obs=y_m)\n",
    "        print(f\"  x_{t}.shape = {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.0155,  0.0770,  0.0630,  0.0503,  0.0369]),\n",
       " tensor([ 0.0111,  0.0054, -0.0019,  0.0091,  0.0018]),\n",
       " tensor([-0.0450, -0.0441, -0.0080, -0.0180, -0.0045]),\n",
       " tensor([ 0.0015, -0.0289, -0.0335, -0.0269, -0.0961]),\n",
       " tensor([0.0211, 0.0777, 0.1038, 0.1159, 0.1292]),\n",
       " tensor([-0.0375, -0.0661, -0.0248, -0.0266, -0.0020]),\n",
       " tensor([-0.0628, -0.0778, -0.0595, -0.0324, -0.0163]),\n",
       " tensor([ 0.0281, -0.0079,  0.0197,  0.0217, -0.0348]),\n",
       " tensor([0.0153, 0.1283, 0.1319, 0.1480, 0.1641]),\n",
       " tensor([0.0356, 0.0199, 0.0267, 0.0146, 0.0140])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random samples\n",
    "data = []\n",
    "for _ in range(10):\n",
    "    data.append(infrequent_step_5(scale_guess, b_guess, epsilon_guess))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m pyro\u001b[38;5;241m.\u001b[39mclear_param_store()\n\u001b[1;32m      4\u001b[0m elbo \u001b[38;5;241m=\u001b[39m TraceEnum_ELBO(max_plate_nesting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m elbo\u001b[38;5;241m.\u001b[39mloss(infrequent_step_5, hmm_guide, data, data_dim\u001b[38;5;241m=\u001b[39m\u001b[43mdata_dim\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_dim' is not defined"
     ]
    }
   ],
   "source": [
    "hmm_guide = AutoNormal(pyro.poutine.block(infrequent_step_5, expose=[\"scale\", \"b\", \"epsilon\"]))\n",
    "\n",
    "pyro.clear_param_store()\n",
    "elbo = TraceEnum_ELBO(max_plate_nesting=1)\n",
    "elbo.loss(infrequent_step_5, hmm_guide, data, data_dim=data_dim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for help online\n",
    "\n",
    "Relevant or related topics/posts:\n",
    "\n",
    "- [What is the difference between a particle filter (sequential Monte Carlo) and a Kalman filter?](https://stats.stackexchange.com/questions/2149/what-is-the-difference-between-a-particle-filter-sequential-monte-carlo-and-a)\n",
    "\n",
    "> Note that Kalman filters by design only deal with Gaussian posterior distributions. Note that the different flavors (extended, unscented, ensemble) just vary in how they estimate the Gaussian in case of nonlinear dynamic/observation models. Particle filters can handle arbitrary arbitrary posteriors, including multi-modal ones. – GeoMatt22  Aug 31 '16.\n",
    "\n",
    "- [Estimating parameters of a dynamic linear model](https://stats.stackexchange.com/questions/4991/estimating-parameters-of-a-dynamic-linear-model)\n",
    "\n",
    "> If you have time varying parameters and want to do things sequentially (filtering), then SMC makes the most sense. MCMC is better when you want to condition on all of the data, or you have unknown static parameters that you want to estimate. Particle filters have issues with static parameters (degeneracy). Darren Wilkinson, Dec 14 '10.\n",
    "\n",
    "- [From Dan Simon's \"Optimal State Estimation\":](https://stats.stackexchange.com/a/2153/226254)\n",
    "\n",
    "> In a linear system with Gaussian noise, the Kalman filter is optimal. In a system that is nonlinear, the Kalman filter can be used for state estimation, but the particle filter may give better results at the price of additional computational effort. In a system that has non-Gaussian noise, the Kalman filter is the optimal linear filter, but again the particle filter may perform better. The unscented Kalman filter (UKF) provides a balance between the low computational effort of the Kalman filter and the high performance of the particle filter.\n",
    "\n",
    "> The particle filter has some similarities with the UKF in that it transforms a set of points via known nonlinear equations and combines the results to estimate the mean and covariance of the state. However, in the particle filter the points are chosen randomly, whereas in the UKF the points are chosen on the basis of a specific algorithm*. Because of this, the number of points used in a particle filter generally needs to be much greater than the number of points in a UKF. Another difference between the two filters is that the estimation error in a UKF does not converge to zero in any sense, but the estimation error in a particle filter does converge to zero as the number of particles (and hence the computational effort) approaches infinity.\n",
    "\n",
    "> *The unscented transformation is a method for calculating the statistics of a random variable which undergoes a nonlinear transformation and uses the intuition (which also applies to the particle filter) that it is easier to approximate a probability distribution than it is to approximate an arbitrary nonlinear function or transformation. See also this as an example of how the points are chosen in UKF.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "true_scale = 0.01\n",
    "true_b = 100\n",
    "true_epsilon = 0.01\n",
    "\n",
    "wp_measurements = [random_shocks(true_scale, true_b, true_epsilon).item()\n",
    "                   for _ in range(1000)]\n",
    "\n",
    "x_minmax = true_scale * true_b * np.array([-1 , 1])\n",
    "\n",
    "plt.hist(wp_measurements, bins=np.linspace(*x_minmax, 51), \n",
    "         density=True)\n",
    "plt.xlim(x_minmax)\n",
    "plt.xlabel('Randomly occuring shocks')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.savefig('plots/wp_samples.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose how much data to provide\n",
    "data = torch.tensor(wp_measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide function\n",
    "\n",
    "The guide function represents the family of distributions we want to consider as our posterior distribution. In this case, it is obvious that the distribution is a Gaussian mixture model with 2 components.\n",
    "\n",
    "The function below is based on example code here: https://pyro.ai/examples/gmm.html\n",
    "\n",
    "This model below allows different scale for each mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2  # Number of mixture components (fixed)\n",
    "\n",
    "@config_enumerate\n",
    "def model(data):\n",
    "    # Global variables.\n",
    "    weights = pyro.sample('weights', dist.Dirichlet(0.5 * torch.ones(K)))\n",
    "    with pyro.plate('components', K):\n",
    "        scales = pyro.sample('scales', dist.LogNormal(0., 2.))\n",
    "        locs = pyro.sample('locs', dist.Normal(0., 10.))\n",
    "\n",
    "    with pyro.plate('data', len(data)):\n",
    "        # Local variables.\n",
    "        assignment = pyro.sample('assignment', dist.Categorical(weights))\n",
    "        pyro.sample('obs', dist.Normal(locs[assignment], scales[assignment]), \n",
    "                    obs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = pyro.optim.Adam({'lr': 0.1, 'betas': [0.8, 0.99]})\n",
    "elbo = TraceEnum_ELBO(max_plate_nesting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before inference we’ll initialize to plausible values. **Mixture models are very succeptible to local modes**. A common approach is choose the best among many randomly initializations, where the cluster means are initialized from random subsamples of the data. Since we’re using an `AutoDelta` guide, we can initialize by defining a custom `init_loc_fn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_loc_fn(site):\n",
    "    if site[\"name\"] == \"weights\":\n",
    "        # Initialize weights to uniform.\n",
    "        return torch.ones(K) / K\n",
    "    if site[\"name\"] == \"scales\":\n",
    "        # Initialise std. dev's. to std. dev of data\n",
    "        return (data.var() * torch.ones(K) / 2).sqrt()\n",
    "    if site[\"name\"] == \"locs\":\n",
    "        # Initialise means to two randomly chosen data points\n",
    "        return data[torch.multinomial(torch.ones(len(data)) / len(data), K)]\n",
    "    raise ValueError(site[\"name\"])\n",
    "\n",
    "def initialize(seed):\n",
    "    global global_guide, svi\n",
    "    pyro.set_rng_seed(seed)\n",
    "    pyro.clear_param_store()\n",
    "    global_guide = AutoDelta(\n",
    "        pyro.poutine.block(model, expose=['weights', 'locs', 'scales']), \n",
    "        init_loc_fn=init_loc_fn\n",
    "    )\n",
    "    svi = SVI(model, global_guide, optim, loss=elbo)\n",
    "    return svi.loss(model, global_guide, data)\n",
    "\n",
    "# Choose the best among 100 random initializations.\n",
    "loss, seed = min((initialize(seed), seed) for seed in range(100))\n",
    "initialize(seed)\n",
    "print('seed = {}, initial_loss = {}'.format(seed, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we’ll collect both losses and gradient norms to monitor convergence. We can do this using PyTorch’s `.register_hook()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register hooks to monitor gradient norms.\n",
    "gradient_norms = defaultdict(list)\n",
    "for name, value in pyro.get_param_store().named_parameters():\n",
    "    value.register_hook(\n",
    "        lambda g, name = name: gradient_norms[name].append(g.norm().item())\n",
    "    )\n",
    "\n",
    "losses, params  = [], []\n",
    "for i in range(200 if not smoke_test else 2):\n",
    "    loss = svi.step(data)\n",
    "    losses.append(loss)\n",
    "    print('.' if i % 100 else '\\n', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find lowest point in training\n",
    "iter_best = np.argmin(losses)\n",
    "iter_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3), dpi=100).set_facecolor('white')\n",
    "plt.plot(losses)\n",
    "plt.xlabel('iters')\n",
    "plt.ylabel('loss')\n",
    "#plt.yscale('log')\n",
    "plt.title('Convergence of SVI');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4), dpi=100).set_facecolor('white')\n",
    "for name, grad_norms in gradient_norms.items():\n",
    "    plt.plot(grad_norms, label=name)\n",
    "plt.xlabel('iters')\n",
    "plt.ylabel('gradient norm')\n",
    "plt.yscale('log')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Gradient norms during SVI');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_estimates = global_guide(data)\n",
    "weights = map_estimates['weights']\n",
    "locs = map_estimates['locs']\n",
    "scales = map_estimates['scales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs, minmax, mean, variance, _, _ = scipy.stats.describe(wp_measurements)\n",
    "\n",
    "X = np.linspace(*minmax ,101)\n",
    "Y1 = weights[0].item() * scipy.stats.norm.pdf(\n",
    "    (X - locs[0].item()) / scales[0].item()\n",
    ")\n",
    "Y2 = weights[1].item() * scipy.stats.norm.pdf(\n",
    "    (X - locs[1].item()) / scales[1].item()\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "ax = axes[0]\n",
    "ax.hist(wp_measurements, bins=61, density=True)\n",
    "ax.set_ylabel('Probability density')\n",
    "ax.grid()\n",
    "ax.set_title('Histogram of Data')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(X, Y1, 'r-', label='Y1')\n",
    "ax.plot(X, Y2, 'b-', label='Y2')\n",
    "ax.plot(X, Y1 + Y2, 'k--', label='Y1+Y2')\n",
    "#ax.plot(data.data.numpy(), np.zeros(len(data)), 'k*')\n",
    "ax.set_title('PDFs of two-component mixture model')\n",
    "ax.set_xlabel('Temperature')\n",
    "ax.set_ylabel('Probability density')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "plt.savefig('plots/weather-dist-est.pdf', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"{'Estimates':27s} {'True Parameters':27s} \")\n",
    "print(\"-\"*80)\n",
    "print(f\"weights = {weights.data.numpy().round(3)}       {true_epsilon = }\")\n",
    "print(f\"locs = {locs.data.numpy().round(3)}      true_mean = 0\")\n",
    "true_scales = np.array([true_scale, true_b*true_scale])\n",
    "print(f\"scales = {scales.data.numpy().round(3)}      {true_scales = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
